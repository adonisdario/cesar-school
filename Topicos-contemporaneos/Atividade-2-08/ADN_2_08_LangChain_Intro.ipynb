{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# OPENAI KEY\n",
        "# sk-proj-g9QWJ5DqRkvmreHpWHOOT3BlbkFJRMBEwJHd5XLLdCUfeAsT"
      ],
      "metadata": {
        "id": "l9U7jdluGUoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvXPkIuI_HgX"
      },
      "outputs": [],
      "source": [
        "# import getpass\n",
        "# import os\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxoJFR2jGOOU",
        "outputId": "15a96f6f-9da3-4bec-d81e-911d56b40cf5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.2.14-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.25)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.3)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Collecting langchain-core<0.4.0,>=0.3.25 (from langchain)\n",
            "  Downloading langchain_core-0.3.28-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting openai<2.0.0,>=1.58.1 (from langchain_openai)\n",
            "  Downloading openai-1.58.1-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain_openai) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain) (3.0.0)\n",
            "Downloading langchain_openai-0.2.14-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.28-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.58.1-py3-none-any.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, openai, langchain-core, langchain_openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.57.4\n",
            "    Uninstalling openai-1.57.4:\n",
            "      Successfully uninstalled openai-1.57.4\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.25\n",
            "    Uninstalling langchain-core-0.3.25:\n",
            "      Successfully uninstalled langchain-core-0.3.25\n",
            "Successfully installed langchain-core-0.3.28 langchain_openai-0.2.14 openai-1.58.1 tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bW__ncB_Hgb"
      },
      "source": [
        "## Modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UF1w4KxN_Hge"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "teVdkv7u_Hgf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a63e8904-b97f-4f5e-b1db-5b9bc2a96123"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Ol√°! Como posso ajudar voc√™ hoje?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None} id='run-67dd73ee-35f9-4549-b4f7-36db79e69e77-0' usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(\"Ol√°\")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "021jXRjq_Hgh"
      },
      "source": [
        "## Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkNaGJgx_Hgh"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpqMqHrA_Hgh"
      },
      "source": [
        "### Templates Simples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YbLd34fB_Hgh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5d650dd-7ae0-477e-cb05-d4a4a5fe37d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[HumanMessage(content='Traduza o seguinte texto para portugu√™s: Artificial Intelligence is the future!', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(\"Traduza o seguinte texto para portugu√™s: {texto}\")\n",
        "\n",
        "prompt = prompt_template.invoke({\"texto\": \"Artificial Intelligence is the future!\"})\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "by2k5PY1_Hgi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65870420-7eb4-4991-b870-101137171845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A Intelig√™ncia Artificial √© o futuro!\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(prompt)\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZCt6KA9_Hgk"
      },
      "source": [
        "### Templates de Mensagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Z_O5YzIv_Hgk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35ecbd03-f506-4960-ec0c-f7893c0e825e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Ol√°, como voc√™ est√°?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 36, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None} id='run-3479cb20-08a1-45f5-93aa-9b45494aabcb-0' usage_metadata={'input_tokens': 36, 'output_tokens': 6, 'total_tokens': 42, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Voc√™ √© um tradutor de ingl√™s para portugu√™s. Traduza as mensagens que forem enviadas.\"),\n",
        "    HumanMessage(content=\"Hello, how are you?\"),\n",
        "]\n",
        "\n",
        "# messages = [\n",
        "#     (\"system\", \"Voc√™ √© um tradutor de ingl√™s para portugu√™s. Traduza as mensagens que forem enviadas.\"),\n",
        "#     (\"human\", \"Hello, how are you?\"),\n",
        "# ]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4bejryFP_Hgl"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Voc√™ √© um tradutor de {lingua_origem} para {lingua_destino}. Traduza as mensagens que forem enviadas.\"),\n",
        "        (\"user\", \"{texto}\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fxIswwuc_Hgl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c6de8b-3407-46b5-941d-7a278e822bd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='Voc√™ √© um tradutor de ingl√™s para portugu√™s. Traduza as mensagens que forem enviadas.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "prompt = prompt_template.invoke({\n",
        "    \"lingua_origem\": \"ingl√™s\",\n",
        "    \"lingua_destino\": \"portugu√™s\",\n",
        "    \"texto\": \"Hello, how are you?\"\n",
        "})\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7dKZhSOm_Hgl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03576de4-0b7a-4dd8-cefa-71f526492777"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ol√°, como voc√™ est√°?\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(prompt)\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aow3j_mY_Hgm"
      },
      "source": [
        "### Parsers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "WPJFDrYs_Hgn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18228505-69ee-4898-9b08-4b7a05b85732"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta:\n",
            "content='A capital do Rio Grande do Norte √© Natal.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 16, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None} id='run-cd7af89d-ae35-4a8e-b693-9f058007c2b6-0' usage_metadata={'input_tokens': 16, 'output_tokens': 11, 'total_tokens': 27, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "Sa√≠da do parser:\n",
            "A capital do Rio Grande do Norte √© Natal.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "str_parser = StrOutputParser()\n",
        "\n",
        "response = llm.invoke(\"Qual a capital do Rio Grande do Norte?\")\n",
        "output = str_parser.invoke(response)\n",
        "\n",
        "print(\"Resposta:\")\n",
        "print(response)\n",
        "print()\n",
        "print(\"Sa√≠da do parser:\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hCX7ijkA_Hgn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11ddb913-4ab8-4845-dbbf-9789901afd29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resposta:\n",
            "content='Aqui est√° a representa√ß√£o em formato JSON das massas e cargas das part√≠culas que constituem o √°tomo:\\n\\n```json\\n{\\n    \"pr√≥ton\": {\\n        \"massa\": \"1.6726 x 10^-27 kg\",\\n        \"carga\": \"+1 e\"\\n    },\\n    \"n√™utron\": {\\n        \"massa\": \"1.6750 x 10^-27 kg\",\\n        \"carga\": \"0 e\"\\n    },\\n    \"el√©tron\": {\\n        \"massa\": \"9.1094 x 10^-31 kg\",\\n        \"carga\": \"-1 e\"\\n    }\\n}\\n```\\n\\nNeste formato, as massas est√£o expressas em quilogramas (kg) e as cargas em unidades de carga elementar (e).' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 157, 'prompt_tokens': 38, 'total_tokens': 195, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None} id='run-a65759e7-c9e9-4ed3-ba11-3abea0c790b9-0' usage_metadata={'input_tokens': 38, 'output_tokens': 157, 'total_tokens': 195, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "Sa√≠da do parser:\n",
            "{'pr√≥ton': {'massa': '1.6726 x 10^-27 kg', 'carga': '+1 e'}, 'n√™utron': {'massa': '1.6750 x 10^-27 kg', 'carga': '0 e'}, 'el√©tron': {'massa': '9.1094 x 10^-31 kg', 'carga': '-1 e'}}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "json_parser = JsonOutputParser()\n",
        "\n",
        "response = llm.invoke(\"Quais as massas e cargas das part√≠culas que constituem o √°tomo? Responda no formato JSON em que cada chave seja o nome da part√≠cula\")\n",
        "output = json_parser.invoke(response)\n",
        "\n",
        "print(\"Resposta:\")\n",
        "print(response)\n",
        "print()\n",
        "print(\"Sa√≠da do parser:\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwRoVwf8_Hgn"
      },
      "source": [
        "## Encadeamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7XTWDyYN_Hgn"
      },
      "outputs": [],
      "source": [
        "chain = prompt_template | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "nNbOg4Ta_Hgo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cf1d0b2-60b2-483f-8c5a-352373859ccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¬°Las playas de Recife tienen tiburones!\n"
          ]
        }
      ],
      "source": [
        "response = chain.invoke({\n",
        "    \"lingua_origem\": \"ingl√™s\",\n",
        "    \"lingua_destino\": \"espanhol\",\n",
        "    \"texto\": \"As praias de Recife tem tubar√µes!\"\n",
        "})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "N7HyUbxA_Hgo"
      },
      "outputs": [],
      "source": [
        "def translate(texto, lingua_origem, lingua_destino):\n",
        "    response = chain.invoke({\n",
        "        \"lingua_origem\": lingua_origem,\n",
        "        \"lingua_destino\": lingua_destino,\n",
        "        \"texto\": texto\n",
        "    })\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "jOrYoStb_Hgo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "557441aa-c082-4e50-bb95-9f28f7f9d6a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¬°Las playas de Recife tienen tiburones!\n"
          ]
        }
      ],
      "source": [
        "output = translate(\"The beaches of Recife have sharks!\", \"ingl√™s\", \"espanhol\")\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIJTU9VZ_Hgo"
      },
      "source": [
        "## Exerc√≠cios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai_gvcG5_Hgp"
      },
      "source": [
        "### Exerc√≠cio 1\n",
        "Crie uma `chain` que a partir de um t√≥pico informado pelo usu√°rio, crie uma piada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "zfKSZN7Q_Hgp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eb39ce0-48c4-4276-c10c-f13f78c5ea93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Claro! Aqui vai uma piada sobre um papagaio:\n",
            "\n",
            "Um homem entra em uma loja de animais e v√™ um papagaio com um pre√ßo muito baixo. Curioso, ele pergunta ao vendedor por que o papagaio √© t√£o barato.\n",
            "\n",
            "O vendedor responde: \"Ah, esse papagaio vive em um bordel, ent√£o ele aprendeu algumas palavras meio... inusitadas.\"\n",
            "\n",
            "O homem, achando que √© uma boa oportunidade, compra o papagaio. Quando chega em casa, o papagaio olha ao redor e diz: \"Nova casa, nova cliente!\"\n",
            "\n",
            "O homem ri e fala: \"Ok, agora vamos ver o que voc√™ sabe fazer!\"\n",
            "\n",
            "O papagaio responde: \"Ah, voc√™ √© o novo cliente, ent√£o! E quem √© essa senhora?\"\n",
            "\n",
            "A esposa do homem entra na sala e o papagaio diz: \"Oi, querida! Prazer em te conhecer!\"\n",
            "\n",
            "O homem fica chocado, mas a esposa ri e diz: \"Voc√™ √© engra√ßado, papagaio!\"\n",
            "\n",
            "Ent√£o, o papagaio olha para o homem e diz: \"E voc√™? Tamb√©m quer saber o que eu fa√ßo?\"\n",
            "\n",
            "O homem responde: \"N√£o, obrigado. Eu s√≥ quero que voc√™ fique calado!\"\n",
            "\n",
            "O papagaio pisca e diz: \"Voc√™ que manda, chefe! Mas se precisar de um conselheiro, √© s√≥ chamar... depois da hora do expediente!\" \n",
            "\n",
            "E assim, o papagaio se tornou o melhor amigo do homem, mas ele sempre teve que explicar as \"palavras\" para a esposa! üòÇ\n"
          ]
        }
      ],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Voc√™ √© um contador de piadas. Cria uma anedota ou uma hist√≥ria engra√ßada com a sugest√£o que for enviada.\"),\n",
        "        (\"user\", \"{texto}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt_template | llm | StrOutputParser()\n",
        "\n",
        "response = chain.invoke({\n",
        "    \"texto\": \"Conte-me uma piada sobre papagaio.\"\n",
        "})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMfxDULf_Hgp"
      },
      "source": [
        "### Exerc√≠cio 2\n",
        "Crie uma `chain` que classifique o sentimento de um texto de entrada em positivo, neutro ou negativo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Sy1IZj0f_Hgp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98027c20-ea58-4c95-b429-b26aca2eb62a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neutro\n"
          ]
        }
      ],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Classifique o sentimento da frase que o usu√°rio enviar. O sentimento pode ser positivo, neutro ou negativo.\"),\n",
        "        (\"user\", \"{texto}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt_template | llm | StrOutputParser()\n",
        "\n",
        "response = chain.invoke({\n",
        "    \"texto\": \"Recife √© uma cidade com muitos mangues\"\n",
        "})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alcRnOiy_Hgp"
      },
      "source": [
        "### Exerc√≠cio 3\n",
        "Crie uma `chain` que gere o c√≥digo de uma fun√ß√£o Python de acordo com a descri√ß√£o do usu√°rio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "MemfZ4El_Hgp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "374a0f28-20fb-44e5-e1b9-3bffc3437113"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voc√™ pode usar um c√≥digo em Python para contar a quantidade de vogais na palavra \"Recife\". Aqui est√° um exemplo de como fazer isso:\n",
            "\n",
            "```python\n",
            "def contar_vogais(palavra):\n",
            "    vogais = \"aeiouAEIOU\"\n",
            "    contador = 0\n",
            "    for letra in palavra:\n",
            "        if letra in vogais:\n",
            "            contador += 1\n",
            "    return contador\n",
            "\n",
            "palavra = \"Recife\"\n",
            "quantidade_vogais = contar_vogais(palavra)\n",
            "print(f\"A quantidade de vogais na palavra '{palavra}' √©: {quantidade_vogais}\")\n",
            "```\n",
            "\n",
            "Esse c√≥digo define uma fun√ß√£o que percorre a string e conta quantas vogais ela possui. Quando voc√™ execut√°-lo, ele mostrar√° a quantidade de vogais na palavra \"Recife\".\n"
          ]
        }
      ],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Gere um c√≥digo em python a partir da entrada do usu√°rio, mas apenas se o que ele pedir fizer sentido fazer um c√≥digo em python. Caso n√£o fa√ßa sentido, responda normalmente\"),\n",
        "        (\"user\", \"{texto}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt_template | llm | StrOutputParser()\n",
        "\n",
        "response = chain.invoke({\n",
        "    \"texto\": \"Como percorrer a string Recife para contar a quantidade de vogais na palavra?\"\n",
        "})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AYRiA9q_Hgp"
      },
      "source": [
        "### Exerc√≠cio 4\n",
        "Crie uma `chain` que explique de forma simplificada um t√≥pico geral fornecido pelo usu√°rio e, em seguida, traduza a explica√ß√£o para ingl√™s. Utilize dois templates encadeados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "noxKYy64_Hgp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f35f7edc-f433-4ecf-9264-b9d4953d0b69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recife √© a capital do estado de Pernambuco, no Brasil, conhecida por suas praias, cultura rica e o famoso carnaval.\n",
            "\n",
            "Recife is the capital of the state of Pernambuco in Brazil, known for its beaches, rich culture, and famous carnival.\n"
          ]
        }
      ],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Se o usu√°rio pedir informa√ß√µes ou perguntar sobre algum t√≥pico, responda resumidamente em portugu√™s e em seguida traduza para o ingl√™s.\"),\n",
        "        (\"user\", \"{texto}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt_template | llm | StrOutputParser()\n",
        "\n",
        "response = chain.invoke({\n",
        "    \"texto\": \"Recife √© uma cidade de Pernambuco\"\n",
        "})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFrlkCGr_Hgp"
      },
      "source": [
        "### Exerc√≠cio 5 - Desafio\n",
        "Crie uma `chain` que responda perguntas sobre o CESAR School."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "qTLRUVzN_Hgp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23710af9-3912-42de-9c77-37cd8c7a63a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embora eu n√£o tenha piadas, posso te contar que na CESAR School, incentivamos a criatividade e o pensamento fora da caixa. \n",
            "\n",
            "Aqui, voc√™ pode desenvolver suas habilidades em √°reas como design, tecnologia e inova√ß√£o, onde a originalidade √© valorizada. \n",
            "\n",
            "Assim como um papagaio que aprende a falar, nossos alunos aprendem a comunicar suas ideias de forma clara e impactante.\n"
          ]
        }
      ],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Voc√™ √© um rob√¥ da CESAR School. Sua fun√ß√£o √© responder perguntas sobre a CESAR School e apresentar informa√ß√µes resumidas sobre a institui√ß√£o. \" +\n",
        "         \"Caso o usu√°rio n√£o fale algo relacionado √† CESAR School, tente conectar o que o usu√°rio falou com a CESAR School, sem avisar que vai fazer isso. \" +\n",
        "         \"Quebre uma linha no texto a cada 20 palavras de um per√≠odo\"),\n",
        "        (\"user\", \"{texto}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt_template | llm | StrOutputParser()\n",
        "\n",
        "response = chain.invoke({\n",
        "    \"texto\": \"Conte-me uma piada sobre papagaio.\"\n",
        "})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke({\n",
        "    \"texto\": \"H√° vagas para trabalhar na CESAR School? Onde fica localizada a institui√ß√£o? Qual o valor da mensalidade dos cursos de p√≥s gradua√ß√£o?\"\n",
        "})\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePLmhorAMCgs",
        "outputId": "cf3ccd63-47ac-4750-ea94-d555a914dd3a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A CESAR School frequentemente atualiza suas oportunidades de trabalho. \n",
            "\n",
            "Recomendo que voc√™ acesse o site oficial da institui√ß√£o ou a p√°gina de carreiras para \n",
            "\n",
            "verificar as vagas dispon√≠veis. A CESAR School est√° localizada em Recife, Pernambuco, \n",
            "\n",
            "em um ambiente que promove inova√ß√£o e criatividade. Quanto ao valor da mensalidade dos \n",
            "\n",
            "cursos de p√≥s-gradua√ß√£o, isso pode variar dependendo do curso espec√≠fico. Sugiro que \n",
            "\n",
            "voc√™ consulte o site da CESAR School para informa√ß√µes detalhadas sobre os valores \n",
            "\n",
            "atualizados e as modalidades de pagamento.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NhNGu3XIN4GK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}